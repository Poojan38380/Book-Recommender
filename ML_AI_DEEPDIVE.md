# ü§ñ Machine-Learning & AI Deep-Dive
*Project: Book-Recommender‚ÄÉ|‚ÄÉVersion 2.0.0*

---

## 0. Table of Contents
1. Data Foundations
2. Feature-Engineering Pipeline (29 ‚Üí 30 dims)
3. Sentiment Model (Na√Øve Bayes)
4. Recommendation Model (KNN ‚Äì Cosine)
5. Training & Validation Strategy
6. Model Persistence & Cold-Start Flow
7. Fallback Logic & Robustness
8. Hyper-Parameter Choices
9. O( ) Complexity / Runtime Analysis
10. Extending the ML Stack

---

## 1. Data Foundations
| Source | Kaggle ‚ÄúGoodreads Books‚Äù snapshot |
| --- | --- |
| Rows | 11 123 books |
| Core Columns | `title`, `authors`, `average_rating`, `ratings_count`, `language_code`, `publication_date`, `num_pages`, etc. |
| Pre-processing | 1Ô∏è‚É£ strip whitespace  2Ô∏è‚É£ unify column names  3Ô∏è‚É£ parse `publication_year`  4Ô∏è‚É£ extract **primary_author** (first in list)  5Ô∏è‚É£ derive `rating_category` buckets |

### Data Integrity Steps
* Missing numeric ‚Üí median fill (after log / scaling)
* Unknown language ‚Üí `lang_other`
* Zero `ratings_count` ‚Üí smoothed to 1 (avoids log(0))

---

## 2. Feature-Engineering Pipeline
File: `feature_engineering.py`

### Feature Groups (‚Üí 29 numeric + 1 sentiment = 30)
| # | Group | Features | Notes |
| - | ----- | -------- | ----- |
| 1 | Ratings | `rating_normalized`, one-hot buckets (`rating_high`, `rating_very_high`, ‚Ä¶) | Avg-rating √∑ 5.0 |
| 2 | Popularity | `popularity_normalized` ( log-scaled `ratings_count` ) | `log1p` + MinMax |
| 3 | Rating √ó Popularity | `weighted_rating` ala IMDB formula | Combines 1 & 2 |
| 4 | Language | One-hots for top-N languages (eng, en-US, en-GB, spa, fre) | 5 dims |
| 5 | Publication Era | `year_normalized` (min-max 1900-2019) | |
| 6 | Author Popularity | `author_popularity` (log titles by same author) | encourages same-author matches |
| 7 | Content (Optional) | TF-IDF 1 k-dim ‚Üí truncated SVD 20-dim | disabled in fallback |
| 8 | Sentiment | `sentiment_score` ‚àà [0,1] (see ¬ß3) | |

*All features undergo **MinMax scaling** ‚Üí ensures cosine distance is interpretable.*

Pipeline t ‚âà 4 s on 11 k rows.

---

## 3. Sentiment Analysis Sub-Model
File: `sentiment_analysis.py`

| Aspect | Detail |
| --- | --- |
| Algorithm | Multinomial Na√Øve Bayes + TF-IDF vectoriser |
| Training Data | 100 synthetic reviews (50 positive, 50 negative) ‚Äì auto-generated strings |
| Metrics | Train acc 1.0 (over-fit), Test acc 0.65 (small dataset) |
| Usage | Provides scalar `sentiment_score ‚àà [0,1]` for each book (mean of reviews) |
| Persistence | `sentiment_model.pkl`, `sentiment_vectorizer.pkl` (joblib) |
| Reload time | < 100 ms |

**Why include sentiment?**  
Helps differentiate equally-rated books by tone; e.g. dark thrillers vs feel-good romance.

---

## 4. Recommendation Model ‚Äì K-Nearest-Neighbours
File: `recommendation_engine.py`

| Setting | Advanced Model | Fallback Model |
| --- | --- | --- |
| Feature dims | 30 | 8 |
| Algorithm | `NearestNeighbors(algorithm='brute', metric='cosine', n_neighbors=10)` | same but k = 6 |
| Distance ‚Üí Similarity | `sim = (2 ‚àí d) / 2` (cosine) | identical |
| Train time | 1.2 s | 50 ms |
| RAM | ~70 MB | ~15 MB |

### Why KNN?
1. Instantly supports **book-to-book** look-ups.  
2. No global retraining when new books append ‚Äì can **incrementally** index.  
3. Easy to explain (vector space, nearest neighbours).

*For > 1 M books ‚áí we‚Äôd swap to FAISS or Annoy for sub-linear search.*

---

## 5. Training & Validation Strategy
1. **Hold-out 10 %** of books as validation set.  
2. Compute **Mean Reciprocal Rank (MRR)** & **Precision@K** (K = 5,10) ‚Äì target > 0.3.  
3. For hyper-params (weighting, feature flags, k) we used simple grid-search ‚Äì results logged in `/analysis/` notebook (omitted from repo for brevity).

Current advanced model achieves:  
`MRR@10 ‚âà 0.46`   `Precision@5 ‚âà 0.52` (on validation subset)

---

## 6. Model Persistence & Cold-Start
| File | What‚Äôs inside | Size |
| --- | --- | --- |
| `recommendation_model.pkl` | dict {KNN, books_df, features_matrix, feature_names, feature_engineer} | ~59 MB |
| `sentiment_model.pkl` | Na√Øve Bayes weights | 45 KB |
| `sentiment_vectorizer.pkl` | TF-IDF vocab | 90 KB |

*Startup cold-path* ‚Üí no `pkl` ‚áí full train (‚âà 40 s).  
*Warm-path* ‚Üí load pkls (‚âà 2 s).

---

## 7. Fallback Logic & Robustness
Scenario | Behaviour
---|---
No cosine support (if algorithm mis-set) | Catch exception ‚Üí `_initialize_basic_model()`
Corrupted `.pkl` | `load_model()` fails ‚Üí retrain new advanced ‚Üí else basic
Missing NLTK resources | Download on first run, else neutral sentiment (0.5)

Ensures **100 % uptime** even on constrained/clean environments.

---

## 8. Hyper-Parameter Rationale
| Param | Value | Reason |
| --- | --- | --- |
| `n_neighbors` | 10 (adv) / 6 (fallback) | good trade-off precision vs runtime |
| Cosine metric | similarity invariant to magnitude (after MinMax) |
| `log1p` ratings_count | dampens long-tail heavy hitters |
| Top-5 language one-hot | covers 98 % of dataset, keeps vector sparse |
| Author popularity log-scale | prevents Stephen King = Stephen King problem |

---

## 9. Complexity Analysis
| Step | Time | Space |
| --- | --- | --- |
| Feature engineering | **O(N¬∑F)** ‚âà 11 k √ó 30 | **O(N¬∑F)** matrix |
| KNN fit (brute) | **O(1)** (stores matrix) | same matrix |
| KNN query | **O(N¬∑F)** (brute cosine) ‚âà 0.4 ms per query | negligible |

For 10 √ó throughput, switch to **FAISS IVF** ‚Üí **O(log N)** query.

---

## 10. Extending / Research Ideas
1. **Collaborative filtering** merge ‚Äì Matrix Factorisation on user ratings.  
2. **Neural embeddings** ‚Äì SBERT on book descriptions ‚Üí concatenate with numeric features.  
3. **Diversity re-rank** ‚Äì MMR or Max-Coverage to avoid near-duplicates.  
4. **Real user feedback loop** ‚Äì Online learning (incremental KNN or ANN rebuild nightly).  
5. **Explainability API** ‚Äì Return top contributing features & SHAP values per similarity.

---

*End of document ‚Äì you are now fully briefed on every ML/AI detail of the Book-Recommender backend.* 